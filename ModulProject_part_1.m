clc
clear all
close all

%P - входной массив, T - эталонный
P=zeros(100, 21); %массив заданного размера из нулей
T=zeros(3, 100);
x=0:5.e-2:1; %x=[0;1] шаг 0,05

for i=1:100
    c=0.9*rand+0.1; %rand - случайное число из [0;1]
    a=0.9*rand+0.1;
    s=0.9*rand+0.1;
    T(1,i)=c;
    T(2,i)=a;
    T(3,i)=s;
    P(i,:)=c*(x.^2) + a*x + s;
    %P(i,:)=c*exp(-((x-a).^s)); %i-ая строчка
end

P=P'; %транспонирование матрицы

%ФОРМИРОВАНИЕ НЕЙРОСЕТИ
net=newff(minmax(P),[21,15,3],{'logsig' 'logsig' 'purelin'},'trainlm'); %двухслойная однонаправленная сеть
%имя_сети=процедура создания сети(алгоритм нахождения мин. и макс. значений(входной массив), 
%количество нейронов[входной слой, второй слой, выходной слой],
%вид передаточной функции{'входной слой', 'второй слой', 'выходной слой'},'алгоритм обучения')

%ОБУЧЕНИЕ НЕЙРОСЕТИ
net.performFcn='sse' %задаем функцию оценки корректности работы
% sse оценивает сумму квадратичных отклонений выходов нейросети от эталонов
net.trainParam.goal=0.01; %значение квадратичного отклонения - критерия окончания обучения
net.trainParam.epochs=1000; %предельное количество циклов обучения - критерий окончания обучения
[net, tr]=train(net,P,T); %непосредственное обучение

%ТЕСТИРОВАНИЕ НЕЙРОСЕТИ
y=sim(net,P); %обработка тестового массива
[m,b,r]=postreg(y(1,:),T(1,:)); %регрессионый анализ результатов обработки для разных выходных параметров
[m,b,r]=postreg(y(2,:),T(2,:));
[m,b,r]=postreg(y(3,:),T(3,:));

%РЕШЕНИЕ ПОСТАВЛЕННОЙ ЗАДАЧИ
C=0.2 %наперед заданные правильные значения
A=0.8
S=0.7
p=C*(x.^2) + A*x + S;
%p=C*exp(-((x-A).^2/S));
p=p';
Y=sim(net,p); %поиск решения нейросетью
Y